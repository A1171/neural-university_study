{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Переводчик (Seq to Seq) с вниманием."
      ],
      "metadata": {
        "id": "wX90Fc7y2M1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Добро пожаловать на задание уровня Lite.\n",
        "\n",
        "В данном задании требуется на базе текстов добиться ошибки меньше 0.2, причем потребуется это сделать за 5 эпох (Во избежание слишком долгого ожидания обучения).\n",
        "\n",
        "Подсказка: В данной задаче поможет подбор гиперпараметров и можно взять за основу ноутбук практики.\n",
        "\n",
        "Успехов!"
      ],
      "metadata": {
        "id": "LSuU8TGeDbiB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xN57lemlxXkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3dQ7qasxXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LxGv5bdgmN"
      },
      "source": [
        "# модуль для загрузки файлов в colab\n",
        "from google.colab import files \n",
        "\n",
        "# Подключим tensorflow\n",
        "import tensorflow as tf \n",
        "\n",
        "# Подключим токенайзер\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Используем метод для формирования последовательностей одинаковой длины\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "# Загружаем абстрактный класс базовой модели сети от кераса\n",
        "from tensorflow.keras.models import Model \n",
        "\n",
        "# Подключим необходимые слои\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
        "\n",
        "# Подключим оптимайзер\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "\n",
        "# Подключим функцию потерь\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Подключим numpy - библиотеку для работы с массивами данных\n",
        "import numpy as np \n",
        "\n",
        "# Подключим библиотеку для визуализации данных\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Подключим модуль для определения форматирования и местоположения делений на осях графиков\n",
        "import matplotlib.ticker as ticker \n",
        "\n",
        "# Подключим модуль для разбивки данных на обучающую и тестовую выборки\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Подключим модуль для работы с регулярными выражениями\n",
        "import re \n",
        "\n",
        "# Подключим модуль для работы с временем\n",
        "import time\n",
        "\n",
        "# Подключим модуль для работы с операционной системой\n",
        "import os "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjqYknxCqmX8",
        "outputId": "9500cded-8bf3-4ed1-e090-2bf8cd0b8df3"
      },
      "source": [
        "# Скачаем датасет из пар фраз на русском и английском языках \n",
        "!wget  http://www.manythings.org/anki/rus-eng.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-21 14:53:01--  http://www.manythings.org/anki/rus-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3030::6815:5c2c, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14683939 (14M) [application/zip]\n",
            "Saving to: ‘rus-eng.zip’\n",
            "\n",
            "rus-eng.zip         100%[===================>]  14.00M  20.4MB/s    in 0.7s    \n",
            "\n",
            "2022-01-21 14:53:02 (20.4 MB/s) - ‘rus-eng.zip’ saved [14683939/14683939]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VlOfx4jtB9e",
        "outputId": "98568928-91e1-4713-cbd2-6fbd9de19f5c"
      },
      "source": [
        "!unzip -o rus-eng.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rus-eng.zip\n",
            "  inflating: rus.txt                 \n",
            "  inflating: _about.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHBPiDxa9_Mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a680fe9-cae6-43ac-9626-6821dc2decbf"
      },
      "source": [
        "# Проверим распакованные файлы\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_about.txt  rus-eng.zip  rus.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O7TDn54-ATt"
      },
      "source": [
        "# Определим переменную с именем файла с датасетом\n",
        "path_to_file=\"rus.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWA1mr0Rqq_9"
      },
      "source": [
        "def preprocess_sentence(phrases): # Функция принимает содержимое словаря\n",
        "\n",
        "  # Разделяем пробелами слова и знаки препинания(\"А как насчет тебя? \" -> \"А как насчет тебя ? \") \n",
        "  phrases = re.sub(r\"([?.!,;:])\", r\" \\1 \", phrases) # r\" \\1 \" берёт значения 1й группы в скобках; обрамляем указанные символы пробелами\n",
        "\n",
        "  # Заменяем всё на пробелы, за исключением (a-zA-Zа-яёА-ЯЁ?.!,;:)\n",
        "  phrases = re.sub(r\"[^a-zA-Zа-яёА-ЯЁ?.!,;:]+\", \" \", phrases) \n",
        "  \n",
        "  # Получаем строку без случайных лишних пробелов в конце фраз(rstrip удаляет с конца строки)\n",
        "  phrases = phrases.rstrip().strip()      \n",
        "\n",
        "  # Для нашей модели обозначим тегами начало и конец предложения  \n",
        "  phrases = '<start> ' + phrases + ' <end>' \n",
        "\n",
        "  # Функция возвращает предобработанные фразы\n",
        "  return phrases "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKj6v9AIvdSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78f4c65-f5ac-4fa5-c063-68dcadb07ee9"
      },
      "source": [
        "# Покажем пример обработки фразц\n",
        "\n",
        "print(\"Фразы после обработки функцией с т.з. пунктуации примут вид:\") \n",
        "print(preprocess_sentence(\"What about you?\"))                         # Выведем пример до обработки \n",
        "print(preprocess_sentence(\"А как насчет тебя?\"))                      # И после"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фразы после обработки функцией с т.з. пунктуации примут вид:\n",
            "<start> What about you ? <end>\n",
            "<start> А как насчет тебя ? <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaNdSe62eeaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRaVcSowqr9e"
      },
      "source": [
        "# Функция создания датасета\n",
        "\n",
        "def create_dataset(path,          # Путь к файлу\n",
        "                   num_examples): # Необходимый размер датасета \n",
        "\n",
        "  # Открываем файл и разбиваем фразы на отдельные строчки\n",
        "  lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  # В каждой строке словаря разделяем английскую фразу от русской, и пропускаем через функцию предобработки данных\n",
        "  word_pairs = [[preprocess_sentence(phrases) for phrases in l.split('\\t')[0:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  # Вернем пары фраз в виде [по-английски, по-русски]\n",
        "  return zip(*word_pairs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5iuvrauv4KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b644a82f-bc3f-4ad3-cbae-1ecd63651a0a"
      },
      "source": [
        "print(\"Взглянем на пример пары фраз на выходе функции:\")\n",
        "\n",
        "english, russian = create_dataset(path_to_file,40000) # Вызовем функцию для демонстрации\n",
        "print(english[-1])                                    # Выведем последний элемент из списка английских фраз\n",
        "print(russian[-1])                                    # Выведем последний элемент из списка русских фраз"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Взглянем на пример пары фраз на выходе функции:\n",
            "<start> Don t be too long . <end>\n",
            "<start> Не тяните . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5xw1cPZqu5d"
      },
      "source": [
        "# Создадим мини-функцию, возвращающую максимальную длину тензора\n",
        "def max_length(tensor): # Функция принимает на вход тензор(фразы в виде последовательности индексов)\n",
        "\n",
        "  # Вернем значение максимальной длины его элемента \n",
        "  return max(len(t) for t in tensor) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTLSJIakPmEz"
      },
      "source": [
        "def tokenize(language): # Функция принимает текст одного из языков\n",
        "\n",
        "  language_tokenizer = Tokenizer(filters='')               # Вызываем класс Токенизатор, просим его не удалять символы, которые он удаляет по умолчанию\n",
        "  language_tokenizer.fit_on_texts(language)                # \"скармливаем\" ему тексты для обработки и сборки словаря частотности\n",
        "  tensor = language_tokenizer.texts_to_sequences(language) # Разбиваем текст фраз на последовательности индексов\n",
        "  tensor = pad_sequences(tensor, padding='post')           # Делаем последовательности фиксированной длины, заполняя нулями более короткие фразы\n",
        "\n",
        "  # Возвращаем последовательность индексов(назовем ее тензор) и токенизатор\n",
        "  return tensor, language_tokenizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaKt_YpuPmbQ"
      },
      "source": [
        "def load_dataset(path,               # Путь к файлу с текстами\n",
        "                 num_examples=None): # Необходимый объем датасета\n",
        "\n",
        "    # Из исходного текста делаем датасет пар фраз, причём входным языком для сети сделаем русский\n",
        "    targ_language, inp_language = create_dataset(path, num_examples)\n",
        "\n",
        "    # Разбиваем текст на последовательность индексов(назовем ее тензор)\n",
        "    input_tensor, inp_language_tokenizer = tokenize(inp_language)    # Формируем тензоры и токенизатор для русского языка\n",
        "    target_tensor, targ_language_tokenizer = tokenize(targ_language) # Формируем тензоры и токенизатор для английского языка\n",
        "\n",
        "    # Функция вернёт: тензор для русского языка, для английского языка; токенизаторы для русского и английского языков\n",
        "    return input_tensor, target_tensor, inp_language_tokenizer, targ_language_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPfw_d52qxtK"
      },
      "source": [
        "num_examples = 40000 # Выберем 40 тысяч строк(всего в базе около 360тысяч строк, в каждой пара фраз)\n",
        "\n",
        "input_tensor, target_tensor, inp_language_tokenizer, targ_language_tokenizer = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Вычислим максимальные длины тензоров для английского и русского языков, используя ранее заданную функцию\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Создаем тренировочную и тестовую выборки по формуле 80/20\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Xhk8pb5byY"
      },
      "source": [
        "# Визуализируем собранные данные\n",
        "\n",
        "def convert(language_tokenizer,  # Токенайзер\n",
        "            tensor):             # Список индексов слов\n",
        "            \n",
        "  #  Цикл по токенам во фразе\n",
        "  for t in tensor:  \n",
        "    if t!=0:                                                        # Если токен не 0. Т.е. не мусор в конце фразы\n",
        "      print (\"%d ----> %s\" % (t, language_tokenizer.index_word[t])) # Выводи токен и соответствующее слово\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XeriQf5fVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7481ba-6d9c-43fa-f5b7-de644e9f6f28"
      },
      "source": [
        "print (\"Фраза на русском языке; соответствие индекса и слова\")   \n",
        "convert(inp_language_tokenizer, input_tensor_train[0])           # Выведем нулевую пару из русского датасета\n",
        "print ()    \n",
        "\n",
        "print (\"Фраза на английском языке; соответствие индекса и слова\")\n",
        "convert(targ_language_tokenizer, target_tensor_train[0])         # Выведем нулевую пару из агнлийского датасета\n",
        "print ()   \n",
        "                                                      \n",
        "print(\"Рус.яз. тренировочная: \" , len(input_tensor_train), \"фраз; \", \"Анг.яз. тренировочная: \", len(target_tensor_train), \"фраз\")# Выведем статистику по обучающей выборке\n",
        "print(\"Рус.яз. тестовая: \", len(input_tensor_val), \"фраз; \", \"Анг.яз. тестовая: \", len(target_tensor_val), \"фраз\")               # Выведем статистику по тестовой выборке"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фраза на русском языке; соответствие индекса и слова\n",
            "1 ----> <start>\n",
            "4 ----> я\n",
            "7 ----> не\n",
            "2802 ----> поэт\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Фраза на английском языке; соответствие индекса и слова\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "12 ----> m\n",
            "38 ----> not\n",
            "11 ----> a\n",
            "1537 ----> poet\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Рус.яз. тренировочная:  32000 фраз;  Анг.яз. тренировочная:  32000 фраз\n",
            "Рус.яз. тестовая:  8000 фраз;  Анг.яз. тестовая:  8000 фраз\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuEJJsHy9rp5"
      },
      "source": [
        "# Определим постоянные \n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)                     # Укажем что случайно сэмплировать будем по всей длине обучающейся выборки\n",
        "BATCH_SIZE = 256                                          # Указываем размер батча\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE     # Укажем количество шагов в одной эпохе\n",
        "embedding_dim = 8192                                       # Размерность эмбеддинга, векторного пространства\n",
        "units = 2048                                              # Задаем размер слоя(количество нейронов в слое) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB82awfvDB6o"
      },
      "source": [
        "# Задаем размер русского словаря\n",
        "vocab_inp_size = len(inp_language_tokenizer.word_index)+1 \n",
        "\n",
        "# Задаем размер английского словаря\n",
        "vocab_tar_size = len(targ_language_tokenizer.word_index)+1 \n",
        "\n",
        "# Создаём датасет из массивов Numpy(рус и анг тренировочные фразы) со случайной подачей тренировочных сэмплов в процессе обучения\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Передаем в датасет размер батча и указываем, что если в тренировке последний батч окажется неполным, то опустим его\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDgb_Z7y98ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed0796c-0183-4167-d23b-7cdbf4a3b6d2"
      },
      "source": [
        "# Посмотрим на форму примеров полученных батчей\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([256, 12]), TensorShape([256, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJwqiGRCDCu1"
      },
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "  # Конструктор класса \n",
        "  def __init__(self, \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размер пространсва эмбеддинга\n",
        "               enc_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "\n",
        "    super(Encoder, self).__init__()                                   # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                                          # Атрибут возвращает размер батча\n",
        "    self.enc_units = enc_units                                        # Атрибут возвращает размер слоя в кодировщике\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)             # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и с dim=256\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки\n",
        "    self.gru = GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # Метод принимает входную фразу и начальное состояние\n",
        "  def call(self, \n",
        "           x,       # Входная фраза\n",
        "           hidden): # Начальное энкодера\n",
        "    x = self.embedding(x) # входящие тензоры преобразовываются в эмбеддинг\n",
        "    output, state = self.gru(x, initial_state = hidden) #затем пропускаются через GRU и получаем выход + новое состояние\n",
        "\n",
        "    # Выход сети GRU и состояние на выходе\n",
        "    return output, state \n",
        "\n",
        "  # Создаем метод инициализации состояний на скрытых слоях\n",
        "  def initialize_hidden_state(self):\n",
        "\n",
        "    # Вернем тензор из нулей размер батча на размер слоя, итсполбьзуем как начальное состояние энкодера\n",
        "    return tf.zeros((self.batch_sz, self.enc_units)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR4bQIKDFXF"
      },
      "source": [
        "# Создадим модель кодировщика по уже заданным параметрам \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXH74Z-B_Mmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296f8a3e-b44f-44c2-ab12-b61d13540376"
      },
      "source": [
        "# Подадим в качестве примера какой-то сэмпл(Тензор[64, 12]) на вход Encoder'у и визуализируем, что получим\n",
        "sample_hidden = encoder.initialize_hidden_state() #инициализируем начальное скрытое состояние\n",
        "\n",
        "# Даем Encoder'у сэмпл и начальное состояние, и получим выход из сети GRU и состояние на выходе (вызывается метод call класса Encoder)\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Размеры выхода из кодировщика: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Размеры скрытого состояния: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размеры выхода из кодировщика: (batch size, sequence length, units) (256, 12, 2048)\n",
            "Размеры скрытого состояния: (batch size, units) (256, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDTK8eqDHu0"
      },
      "source": [
        "class BahdanauAttention(Model): # Название класса именем создателя механизма Дмитрия Богданова(Bahdanau)\n",
        "\n",
        "  # Создаем конструктор класса\n",
        "  def __init__(self, \n",
        "               units):                        # Число нейронов \n",
        "\n",
        "    super(BahdanauAttention, self).__init__() # Даем возможность использовать и исполнять методы класса-родителя в классе потомке\n",
        "    self.W1 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.W2 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.V =  Dense(1)                        # Создаем Dense с числом нейронов =1\n",
        "\n",
        "  # Метод принимает состояние и выход энкодера ----------------------------------\n",
        "  \n",
        "  def call(self, \n",
        "           hidden_state, # Состояние энкодера\n",
        "           values):      # Выход энкодера\n",
        "    # Форма состояния на скрытом слое (batch_size, hidden size)\n",
        "    # Форму состояния на каждом такте увеличим до (batch_size, 1, hidden size)\n",
        "    # Добавляем это для того, чтобы получить оценку\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
        "    print(hidden_state.shape)\n",
        "    print(values.shape)\n",
        "    print(hidden_with_time_axis.shape)\n",
        "    print(values[1,11,5])\n",
        "    print(hidden_with_time_axis[1,0,5])\n",
        "    # Форма оценки score (размер батча, макс.длина слов на входе, 1), однёрка в конце, чтобы применить self.V\n",
        "    # До применения self.V оценка была бы (размер батча, макс.длина слов на входе, количество нейронов в слое)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    print(self.W1(values).shape)\n",
        "    print(self.W2(hidden_with_time_axis).shape)\n",
        "    print((self.W1(values) + self.W2(hidden_with_time_axis)).shape)\n",
        "    print(score.shape)\n",
        "    \n",
        "    # К полученной оценке применим Софтмакс, который покажет вероятность полезности от 0 до 1 для каждого слова в фразе для декодера\n",
        "    # Форма оценки score - (размер батча, макс.длина слов на входе, 1); Софтмакс применяем к оси \"макс.длина слов\"\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    print(attention_weights.shape)\n",
        "    \n",
        "    # Построим вектор контекста \n",
        "    context_vector = attention_weights * values # Веса внимания перемножим со значениями(выхода из кодировщика)\n",
        "    print(context_vector.shape)\n",
        "    # Сумму также применяем по оси \"макс.длина слов на входе\"\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) # Размеры вектора контекста после суммирования будут (размер батча, размер слоя)\n",
        "    print(context_vector.shape)\n",
        "\n",
        "    # Возвращает вектор контекста и веса внимания\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUv-DSsDDKGl",
        "outputId": "67a72800-448b-4a09-eefb-806f5cd0738a"
      },
      "source": [
        "# Проверим, как работает слой\n",
        "attention_layer = BahdanauAttention(10)\n",
        "\n",
        "# Подадим на вход слою внимания выход из Encodera и его состояние, и получим значение и веса внимания\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Размеры значения внимания: (размер батча, размер слоя) {}\".format(attention_result.shape))\n",
        "print(\"Размеры весов внимания: (размер батча, длина последовательности, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "tf.Tensor(0.062585235, shape=(), dtype=float32)\n",
            "tf.Tensor(0.062585235, shape=(), dtype=float32)\n",
            "(256, 12, 10)\n",
            "(256, 1, 10)\n",
            "(256, 12, 10)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Размеры значения внимания: (размер батча, размер слоя) (256, 2048)\n",
            "Размеры весов внимания: (размер батча, длина последовательности, 1) (256, 12, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHTEgDoDNBC"
      },
      "source": [
        "class Decoder(Model):\n",
        "\n",
        "  # Создадим конструктор класса\n",
        "  def __init__(self,   \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размерность пространства эмбеддинга\n",
        "               dec_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "    super(Decoder, self).__init__()                       # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                              # Атрибут возвращает размер батча\n",
        "    self.dec_units = dec_units                            # Атрибут возвращает размер слоя в декодере(кол-во нейронов)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim) # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и (dim=256) на выходе\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки    \n",
        "    self.gru = GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = Dense(vocab_size) # Атрибут вызовет полносвязный слой с размером словаря\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units) #атрибут подключит механизм внимания, описанный ранее\n",
        "\n",
        "\n",
        "  def call(self, \n",
        "           x,           # Начальный токен\n",
        "           hidden,      # Состояние  энкодера\n",
        "           enc_output): # Выход энкодера\n",
        "\n",
        "    # Enc_output размеры (batch_size, max_length, hidden_size - размер батча, макс.длина фраз, разм.скр.слоя)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    print('Decoder')\n",
        "    print(context_vector.shape)\n",
        "    print(attention_weights.shape)\n",
        "    \n",
        "    # Входящий тензор слова пропускаем через эмбеддинг (получаем размеры batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    print(x.shape)\n",
        "    \n",
        "    # Дальше конкатенируем с вектором контекста (получаем размеры batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    print(tf.expand_dims(context_vector, 1).shape)\n",
        "    print(x.shape)\n",
        "    \n",
        "    # Сконкатенированный вектор передаем  в GRU и получаем выход с декодера и состояние\n",
        "    output, state = self.gru(x)\n",
        "    print(output.shape)\n",
        "    print(state.shape)\n",
        "\n",
        "    # Output размеры (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    print(output.shape)\n",
        "\n",
        "    # Пропускаем через полносвязный слой\n",
        "    x = self.fc(output) #output размеры (batch_size, vocab)\n",
        "    print(x.shape)\n",
        "\n",
        "    # Вернем выходную фразу, вектор состояния, веса внимания\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b1yEPu3DPqp",
        "outputId": "c79f8bdc-04f5-470c-b89a-6e5fd392150f"
      },
      "source": [
        "# Проверим работу декодера, подав на вход случайный массив с нужной размерностью\n",
        "# Создали декодер с параметрами(размер анг.словаря, размерность эмбеддинга, кол-во нейронов, размер батча)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Подаём на вход случайный массив с нужной размерностью, состояние и выход с кодировщика\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((256, 1)), sample_hidden, sample_output)\n",
        "print ('Размер выхода с декодера: (размер батча, размер словаря) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "tf.Tensor(0.062585235, shape=(), dtype=float32)\n",
            "tf.Tensor(0.062585235, shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "Размер выхода с декодера: (размер батча, размер словаря) (256, 4271)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h7uiC6LDVRK"
      },
      "source": [
        "# Выбираем оптимайзер Adam\n",
        "optimizer = Adam()#learning_rate=0.1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YVxy08qDR-5"
      },
      "source": [
        "# Используем SparseCategoricalCrossentropy, к-я может работать с некатегориальными лейблами\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # Выбираем функцию потерь\n",
        "\n",
        "def loss_function(real, pred):                       # Запишем функцию потерь, на вход подаем фактический и предсказанный результат\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # Найдем маску, которая уберет нулевые значения индексов в конце фразы\n",
        "  loss_ = loss_object(real, pred)                    # Фактические и предсказанные результаты передаем в SparseCategoricalCrossentropy и получаем ошибку\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)            # Согласуем тип маски с типом потерь\n",
        "  loss_ *= mask                                      # Накидываем \"маску\" которая оставит для работы ненулевые значения\n",
        "  \n",
        "  # Вернем reduce_mean - среднее любого выбранного тензора\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHr21E05q0NN"
      },
      "source": [
        "# Сохраняем процесс обучения модели чекпоинтами тензорфлоу\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'                                               # Даем ссылку на директорию\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")                                # Добавляем префикс \"ckpt\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder) # Сохраняем состояния/показатели оптимизатора и моделей"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCNvA66Jq7nE"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp,         # Входная фраза\n",
        "               targ,        # Точный перевод\n",
        "               enc_hidden): # Состояния энкодера\n",
        "\n",
        "  # Создаем переменную, в которую будем записывать ошибку\n",
        "  loss = 0                             \n",
        "\n",
        "  # Все операции по вычислению градиента записываются на ленту(tape) и мы получаем к ним доступ\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Передаем тензор и начальное состояние в кодировщик и получим выход и состояние на выходе\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # Передадим это состояние декодеру\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # Передаем в качестве входа в декодер индекс токена \"<start>\"\n",
        "    dec_input = tf.expand_dims([targ_language_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Техника \"Teacher forcing\" - подаем предыдущее выходное слово на вход следущего в декодере. Targ.shape[64, 9]\n",
        "\n",
        "    for t in range(1, targ.shape[1]): #для каждого слова из английской фразы\n",
        "\n",
        "      # Передаем в обработку декодеру начальный токен, состояние на выходе из кодера, и выход из кодера\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) # Получаем от декодера предсказание и обновленное состояние\n",
        "\n",
        "      # Обновляем ошибку для текущих предсказаний\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Используем \"Teacher forcing\"\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # Получаем ошибку на батче . Targ.shape[64, 9]. Делим на 9\n",
        "  batch_loss = (loss / int(targ.shape[1])) \n",
        "\n",
        "  # Создаем список переменных, для которых TensorFlow будет вычислять градиенты\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables # создаем переменные, для которых TensorFlow будет вычислять градиенты\n",
        "\n",
        "  # Отслеживаем градиент\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Корректируем веса\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Функция обучения вернет ошибку на батче\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Tpy6Cxq-V3",
        "outputId": "86856660-cba4-40f5-a75f-3a6b7bd24ee0"
      },
      "source": [
        "EPOCHS = 5 # устанавливаем количество эпох\n",
        "\n",
        "for epoch in range(EPOCHS): # Цикл по каждой эпохе\n",
        "  start = time.time() # Запомним время начала эпохи\n",
        "\n",
        "  progbar = tf.keras.utils.Progbar(target=steps_per_epoch, stateful_metrics=[\n",
        "                                     'batch_loss','accuracy'], unit_name='batch')        # Создадим индикатор прогресс обучения\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state() # Задаем начальное состояние на скрытом слое encodera \n",
        "  total_loss = 0                                 # Начальное значение итоговой ошибки\n",
        "\n",
        "  # Для батча, входного и выходного тензора на каждом шаге эпохи\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden) # Передадим в функцию тензоры и состояние в кодировщике, обучим и получим ошибку на батче\n",
        "    total_loss += batch_loss                       # Добавим ее в итоговую ошибку\n",
        "    progbar.update(                                # Обновим состояние индикатора обучения\n",
        "            batch + 1, values=[('batch_loss', batch_loss)])\n",
        "\n",
        "\n",
        "  # Каждые 10 эпох будем сохранять чекпоинты\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  # Выведем показатели после каждой эпохи\n",
        "  print('Эпоха {} Ошибка {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch)) # Выведем номер эпохи и потери\n",
        "  print('Время на 1 эпоху {} сек'.format(round(time.time() - start), 1))          # Выведем длительность обучения этой эпохи"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_1:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_2:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_3:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_4:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_5:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_6:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_7:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_8:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_9:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_10:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_11:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_12:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_13:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_14:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_15:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_1:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_2:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_3:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_4:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_5:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_6:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_7:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_8:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_9:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_10:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_11:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_12:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_13:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "(256, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_14:0\", shape=(), dtype=float32)\n",
            "Tensor(\"decoder_13/bahdanau_attention_26/strided_slice_15:0\", shape=(), dtype=float32)\n",
            "(256, 12, 2048)\n",
            "(256, 1, 2048)\n",
            "(256, 12, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 12, 1)\n",
            "(256, 12, 2048)\n",
            "(256, 2048)\n",
            "Decoder\n",
            "(256, 2048)\n",
            "(256, 12, 1)\n",
            "(256, 1, 8192)\n",
            "(256, 1, 2048)\n",
            "(256, 1, 10240)\n",
            "(256, 1, 2048)\n",
            "(256, 2048)\n",
            "(256, 2048)\n",
            "(256, 4271)\n",
            "125/125 [==============================] - 137s 1s/batch - batch_loss: 1.2447\n",
            "Эпоха 1 Ошибка 1.8540\n",
            "Время на 1 эпоху 142 сек\n",
            "125/125 [==============================] - 126s 1s/batch - batch_loss: 0.8299\n",
            "Эпоха 2 Ошибка 0.9927\n",
            "Время на 1 эпоху 142 сек\n",
            "125/125 [==============================] - 127s 1s/batch - batch_loss: 0.5062\n",
            "Эпоха 3 Ошибка 0.5524\n",
            "Время на 1 эпоху 142 сек\n",
            "125/125 [==============================] - 127s 1s/batch - batch_loss: 0.3078\n",
            "Эпоха 4 Ошибка 0.2947\n",
            "Время на 1 эпоху 142 сек\n",
            "125/125 [==============================] - 127s 1s/batch - batch_loss: 0.1663\n",
            "Эпоха 5 Ошибка 0.1719\n",
            "Время на 1 эпоху 142 сек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX4koS1irAnJ"
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "    # Создаем начальные настройки графика внимания\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp)) \n",
        "    \n",
        "    # Предобрабатываем предложение\n",
        "    sentence = preprocess_sentence(sentence) \n",
        "\n",
        "    inputs = [inp_language_tokenizer.word_index[i] for i in sentence.split(' ')]   # Преобразовываем в послед-ть индексов\n",
        "    inputs = pad_sequences([inputs], maxlen=max_length_inp, padding='post')        # Делаем паддинг\n",
        "    inputs = tf.convert_to_tensor(inputs)                                          # Конвертируем в тф тензор\n",
        "\n",
        "    result = ''                                                                    # Сюда запишем результат\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]                                                # Задаем начальное состояние\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)                                  # Передаем его и входной тензор и получаем выход с кодера и состояние\n",
        "\n",
        "    dec_hidden = enc_hidden                                                        # Состояние кодера передаем в декодер\n",
        "    dec_input = tf.expand_dims([targ_language_tokenizer.word_index['<start>']], 0) # Передаем на вход декодеру <start> в виде индекса\n",
        "\n",
        "    for t in range(max_length_targ):                                               # Идем по макс.длине фраз выходного языка(анг)\n",
        "        # Прогоняем через декодер входящий тензор, состояние с выхода кодера, выход с кодера\n",
        "        # Получаем результат предсказания, обновленное состояние, и веса внимания\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # Сохраняем веса внимания для графика\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        # Аргмаксом вытаскиваем предсказанное слово\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # Результат конвертируем из индекса в слово и сохраняем в result = ''\n",
        "        result += targ_language_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        # Если предсказанное слово - <end>, то останавливаемся, возвращаем результаты, выводим на графике\n",
        "        if targ_language_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # Педсказанное значение подается обратно в модель\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # Вернем перевод, входную фразу и веса внимания\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It8ZVMbHrFYj"
      },
      "source": [
        "def plot_attention(attention,           # Веса внимания\n",
        "                   sentence,            # Исходная фраза\n",
        "                   predicted_sentence): # Предсказаные перевод\n",
        "  \n",
        "    fig = plt.figure(figsize=(10,10))                                   # Зададим размер \n",
        "    ax = fig.add_subplot(1, 1, 1)                                       # Добавим 1 картинку\n",
        "    ax.matshow(attention, cmap='viridis')                               # Нарисуем 2d матрицу\n",
        "    fontdict = {'fontsize': 14}                                         # Зададим размер надписей\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90) # Добавим надпись по горизонтальной оси\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)    # Добавим надпись по вертикальной оси\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))               # Зададим форматирование делений на осях графиков\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))               # Зададим форматирование делений на осях графиков\n",
        "    plt.show()                                                          # Отрисуем изображение"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8G96tWrHmn"
      },
      "source": [
        "def translate(sentence): # Функция принимает предложение и выводит результат с визуализацией\n",
        "    result, sentence, attention_plot = evaluate(sentence)  # Отдадим фразу. Получим перевод, входную фразу,  веса внимания\n",
        "\n",
        "    print('Входящая фраза: %s' % (sentence))          # Выведем входную фразу \n",
        "    print('Предсказанный перевод: {}'.format(result)) # Выведем полученный перевод\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))] # Возьмем весы внимания, только для слов во фразах. Хвосты не смотрим\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))              # Выведем веса внимания"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vsbPAP5rJle",
        "outputId": "b9900e57-24a5-4ae9-ae00-b402c24a3c46"
      },
      "source": [
        "# Воспроизведём последний сохранённый чекпоинт\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd976418b50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Карты распределения внимания."
      ],
      "metadata": {
        "id": "jgGIGrJ42FpM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "bBi0SqkJrMeK",
        "outputId": "0a2600e9-a9ce-4d92-bf6e-30d05f8d4383"
      },
      "source": [
        "translate('давайте дружить')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> давайте дружить <end>\n",
            "Предсказанный перевод: let s sing . <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "X2VVb9LHrOv8",
        "outputId": "299270ce-4f0b-46ef-ea7f-fcc9eba5792e"
      },
      "source": [
        "translate('у тебя всё хорошо')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> у тебя всё хорошо <end>\n",
            "Предсказанный перевод: you re all right . <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "uWC8-1DDrPQ1",
        "outputId": "bc411cd1-8d81-4556-e78b-a1e5e457e32a"
      },
      "source": [
        "translate('у тебя всё хорошо?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> у тебя всё хорошо ? <end>\n",
            "Предсказанный перевод: are you okay ? <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nW4YFl8ixXrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_1o22k_xXui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}