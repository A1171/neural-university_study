{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Чат бот с вниманием.\n",
        "\n"
      ],
      "metadata": {
        "id": "-V87uUR60hUI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XhodKqwLjQ6"
      },
      "source": [
        "#Добро пожаловать на задание уровня Pro.\n",
        "\n",
        "В данном задании требуется на базе для чат-бота сравнить модель seq2seq и seq2seq с вниманием\n",
        "\n",
        "Успехов!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LxGv5bdgmN"
      },
      "source": [
        "# модуль для загрузки файлов в colab\n",
        "from google.colab import files \n",
        "\n",
        "# Подключим tensorflow\n",
        "import tensorflow as tf \n",
        "\n",
        "# Импортируем слои\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, GlobalMaxPooling1D, LSTM, GRU, Bidirectional, Embedding, Input, Concatenate, Attention\n",
        "\n",
        "# Импортируем модели\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "\n",
        "# Подключим токенайзер\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Используем метод для формирования последовательностей одинаковой длины\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "# Импортируем оптимизаторы\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "\n",
        "# Подключим функцию потерь\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Подключим numpy - библиотеку для работы с массивами данных\n",
        "import numpy as np \n",
        "\n",
        "# Импортируем pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Подключим библиотеку для визуализации данных\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Подключим модуль для определения форматирования и местоположения делений на осях графиков\n",
        "import matplotlib.ticker as ticker \n",
        "\n",
        "# Подключим модуль для разбивки данных на обучающую и тестовую выборки\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Импортируем методы для отображения модели и преобразования в ОНЕ\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "# Подключим модуль для работы с регулярными выражениями\n",
        "import re \n",
        "\n",
        "# Подключим модуль для работы с временем\n",
        "import time\n",
        "\n",
        "# Подключим модуль для работы с операционной системой\n",
        "import os \n",
        "\n",
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подключаем диск\n",
        "gdown.download('https://storage.googleapis.com/datasets_ai/Advanced/3_seq2seq/9kdialogs.txt', None, quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mTELieUaKiyl",
        "outputId": "8cf402cb-448e-4c73-bf2d-539275c2b1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'9kdialogs.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O7TDn54-ATt"
      },
      "source": [
        "# Определим переменную с именем файла с датасетом\n",
        "path_to_file=\"9kdialogs.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwJZiSf9tgni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_file, 'r', encoding='utf-8') as f: # Открываем файл словаря в режиме чтения\n",
        "    lines = f.read().split('\\n')                  # Читаем весь файл, режем на строки"
      ],
      "metadata": {
        "id": "h9flWZJqpOeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lines[0:10])"
      ],
      "metadata": {
        "id": "9A28UwQspOhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31aaa562-2e7c-416b-b0f0-e41ea2e992a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['- - Как вы можете быть таким уверенным?', '- Элементарно.', '', '- - А что делать будем?', '- Ждать.', '- - Надеюсь, не до первой звезды?', '- Я тоже на это надеюсь.', '', '- - За что?', '- Вы знаете за что!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence to sequence"
      ],
      "metadata": {
        "id": "I9KjaTL0RVe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_replacer(s):  \n",
        "\n",
        "    ''' Функция для удаления пробелов перед знаками препинания\n",
        "\n",
        "        Args: строка или список строк\n",
        "\n",
        "        Returns: строка или список строк\n",
        "    '''\n",
        "\n",
        "    if isinstance(s,str): # Если получили строку\n",
        "\n",
        "        # Убираем перед знаками препинания пробел и возвращаем\n",
        "        return s.replace('- ','').replace('- ','').replace(' .','.').replace(' ,',',').replace(' !','!').replace(' ?','?')\n",
        "\n",
        "    if isinstance(s,list): # Если получили список\n",
        "        ou=[]              # Заготовим пустой список\n",
        "\n",
        "        for l in s:        # Цикл по строкам из списка\n",
        "            ou.append(l.replace('- ','').replace('- ','').replace(' .','.').replace(' ,',',').replace(' !','!').replace(' ?','?')) # Убираем перед знаками препинания пробел и возвращаем\n",
        "\n",
        "\n",
        "        # Вернем список строк\n",
        "        return ou   "
      ],
      "metadata": {
        "id": "FLjbYuIYuSNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TStL9Lfi0wIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание выборки."
      ],
      "metadata": {
        "id": "-WNaxYVP0wKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Цикл по строкам\n",
        "PrevLine=''\n",
        "Questions=[]\n",
        "Answers=[]\n",
        "for i,line in enumerate(lines):\n",
        "    \n",
        "    if i>0 and line!='' and PrevLine!='':\n",
        "      Questions.append(my_replacer(PrevLine))\n",
        "      Answers.append(my_replacer(line))\n",
        "    PrevLine=line"
      ],
      "metadata": {
        "id": "66NZ2RijstEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Questions[:4],Answers[:4])"
      ],
      "metadata": {
        "id": "BI_lKSwXpOkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899e024d-3000-4908-f4a6-98dca6973f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Как вы можете быть таким уверенным?', 'А что делать будем?', 'Ждать.', 'Надеюсь, не до первой звезды?'] ['Элементарно.', 'Ждать.', 'Надеюсь, не до первой звезды?', 'Я тоже на это надеюсь.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Answers_SE = ['<START> ' + s + ' <END>' for s in Answers]\n",
        "#Questions_SE = ['<START> ' + s + ' <END>' for s in Questions]"
      ],
      "metadata": {
        "id": "j59bX_NEu0In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим токенайзер \n",
        "tokenizer = Tokenizer(filters='\"#$%&()*+-/;<=>@[\\\\]^_`{|}~\\t\\n',split=' ')  \n",
        "\n",
        "# Загружаем в токенизатор список фраз для сборки словаря частотности\n",
        "tokenizer.fit_on_texts(Questions + Answers_SE)         \n",
        "\n",
        "# Список с cодержимым словаря\n",
        "vocabularyItems = list(tokenizer.word_index.items())    \n",
        "\n",
        "# Размер словаря\n",
        "vocabularySize = len(vocabularyItems)+1        \n",
        "\n",
        "# Выведем фрагмент и размер словаря\n",
        "print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))       \n",
        "print( 'Размер словаря : {}'.format(vocabularySize))             "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndhJycp_vaOv",
        "outputId": "0ee8a7a0-6fe4-42b7-e2d7-689b78bd56d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фрагмент словаря : [('start', 1), ('end', 2), ('не', 3), ('что', 4), ('а', 5), ('ты', 6), ('в', 7), ('я', 8), ('вы', 9), ('это', 10), ('и', 11), ('как', 12), ('на', 13), ('у', 14), ('с', 15), ('где', 16), ('да.', 17), ('нет.', 18), ('кто', 19), ('он', 20), ('что?', 21), ('так', 22), ('же', 23), ('все', 24), ('то', 25), ('ну', 26), ('да', 27), ('за', 28), ('мне', 29), ('да,', 30), ('нет,', 31), ('мы', 32), ('тебя', 33), ('тебе', 34), ('чего', 35), ('что,', 36), ('вас', 37), ('меня', 38), ('вам', 39), ('еще', 40), ('куда', 41), ('ну,', 42), ('по', 43), ('она', 44), ('его', 45), ('знаю.', 46), ('это?', 47), ('чем', 48), ('может,', 49), ('к', 50)]\n",
            "Размер словаря : 16759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем текст входных фраз на последовательности индексов\n",
        "tokenized_Questions = tokenizer.texts_to_sequences(Questions)\n",
        "tokenized_Answers_SE = tokenizer.texts_to_sequences(Answers_SE)\n",
        "\n",
        "# Уточняем длину самой длинной фразы\n",
        "maxLen_Questions = max([ len(x) for x in tokenized_Questions])\n",
        "maxLen_Answers_SE = max([ len(x) for x in tokenized_Answers_SE])\n",
        "\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие фразы\n",
        "padded_Questions = np.array(pad_sequences(tokenized_Questions, maxlen=maxLen_Questions, padding='post'))\n",
        "padded_Answers_SE = np.array(pad_sequences(tokenized_Answers_SE, maxlen=maxLen_Answers_SE, padding='post'))\n",
        "\n",
        "\n",
        "for i in range(len(tokenized_Answers_SE)) :                  # Для разбитых на последовательности ответов\n",
        "    tokenized_Answers_SE[i] = tokenized_Answers_SE[i][1:]          # Избавляемся от тега <START>\n",
        "padded_Answers_E = np.array(pad_sequences(tokenized_Answers_SE, maxlen=maxLen_Answers_SE, padding='post'))\n",
        "\n",
        "\n",
        "# Выведем на экран\n",
        "print('Пример оригинального текста: {}'.format(Answers_SE[100])) \n",
        "print('Пример eng                         : {}'.format(padded_Answers_SE[100]))         \n",
        "print('Пример eng SE                         : {}'.format(padded_Answers_E[100]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P2AydwMvur-",
        "outputId": "d9dfb6ad-46ba-4557-f6fa-79e2dd536fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример оригинального текста: <START> Здесь, здесь. <END>\n",
            "Пример eng                         : [  1 654 150   2   0   0   0   0   0   0   0   0   0   0]\n",
            "Пример eng SE                         : [654 150   2   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим энкодер \n",
        "\n",
        "encoderInputs_t1 = Input(shape=(None , ))                                             # Добавим входной слой\n",
        "encoderEmbedding_t1 = Embedding(vocabularySize, 200 , mask_zero=True)(encoderInputs_t1)  # Добавим эмбеддинг\n",
        "encoderOutputs_t1, state_h_t1 , state_c_t1 = LSTM(200, return_state=True)(encoderEmbedding_t1) # Добавим LSTM\n",
        "encoderStates_t1 = [state_h_t1, state_c_t1]                                                 # Соберем выходы lstm  в список    "
      ],
      "metadata": {
        "id": "Semzo_Sy4eM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим декодер \n",
        "\n",
        "decoderInputs_t1 = Input(shape=(None, ))                                                # Добавим входной слой\n",
        "decoderEmbedding_t1 = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs_t1)    # Добавим эмбеддинг\n",
        "\n",
        "decoderLSTM_t1 = LSTM(200, return_state=True, return_sequences=True)\n",
        "#decoderOutputs = decoderLSTM1 (decoderEmbedding, initial_state=encoderStates)\n",
        "decoderOutputs_t1 , _ , _ = decoderLSTM_t1 (decoderEmbedding_t1, initial_state=encoderStates_t1) # Прогоним выход embedding через LSTM\n",
        "decoderDense_t1 = Dense(vocabularySize, activation='softmax')                           # Создадим dense слой\n",
        "output_t1 = decoderDense_t1 (decoderOutputs_t1)                                               # Прогоним  выход LSTM через DENSE"
      ],
      "metadata": {
        "id": "kfWDal-o4m9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Собираем модель\n",
        "\n",
        "model_1LSTM = Model([encoderInputs_t1, decoderInputs_t1], output_t1)\n",
        "model_1LSTM.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy')\n",
        "print(model_1LSTM.summary()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo8eM41t4nCY",
        "outputId": "6a8465c3-20a3-46b0-f89f-144dce83e783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 200)    3351800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    3351800     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 16759)  3368559     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,713,759\n",
            "Trainable params: 10,713,759\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1LSTM.fit([padded_Questions , padded_Answers_SE], padded_Answers_E, batch_size=256, epochs=100) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABdmrurx5JZk",
        "outputId": "4b157795-9f61-426e-eec3-098ee8528a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "42/42 [==============================] - 13s 51ms/step - loss: 2.3692\n",
            "Epoch 2/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 1.9137\n",
            "Epoch 3/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 1.7776\n",
            "Epoch 4/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 1.6989\n",
            "Epoch 5/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 1.6488\n",
            "Epoch 6/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.6084\n",
            "Epoch 7/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 1.5750\n",
            "Epoch 8/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.5458\n",
            "Epoch 9/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.5192\n",
            "Epoch 10/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.4941\n",
            "Epoch 11/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.4690\n",
            "Epoch 12/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.4446\n",
            "Epoch 13/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.4206\n",
            "Epoch 14/100\n",
            "42/42 [==============================] - 2s 52ms/step - loss: 1.3969\n",
            "Epoch 15/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.3746\n",
            "Epoch 16/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.3520\n",
            "Epoch 17/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.3294\n",
            "Epoch 18/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.3073\n",
            "Epoch 19/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.2855\n",
            "Epoch 20/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.2631\n",
            "Epoch 21/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.2405\n",
            "Epoch 22/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.2168\n",
            "Epoch 23/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.1939\n",
            "Epoch 24/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.1717\n",
            "Epoch 25/100\n",
            "42/42 [==============================] - 2s 51ms/step - loss: 1.1496\n",
            "Epoch 26/100\n",
            "42/42 [==============================] - 2s 51ms/step - loss: 1.1271\n",
            "Epoch 27/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.1056\n",
            "Epoch 28/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 1.0841\n",
            "Epoch 29/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.0622\n",
            "Epoch 30/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.0417\n",
            "Epoch 31/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.0207\n",
            "Epoch 32/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 1.0002\n",
            "Epoch 33/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.9798\n",
            "Epoch 34/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.9589\n",
            "Epoch 35/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.9385\n",
            "Epoch 36/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.9185\n",
            "Epoch 37/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.8990\n",
            "Epoch 38/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.8791\n",
            "Epoch 39/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.8596\n",
            "Epoch 40/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.8407\n",
            "Epoch 41/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.8216\n",
            "Epoch 42/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.8017\n",
            "Epoch 43/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.7831\n",
            "Epoch 44/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.7640\n",
            "Epoch 45/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.7457\n",
            "Epoch 46/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.7272\n",
            "Epoch 47/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.7086\n",
            "Epoch 48/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.6907\n",
            "Epoch 49/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.6734\n",
            "Epoch 50/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.6551\n",
            "Epoch 51/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.6371\n",
            "Epoch 52/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.6210\n",
            "Epoch 53/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.6035\n",
            "Epoch 54/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.5875\n",
            "Epoch 55/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.5704\n",
            "Epoch 56/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 0.5558\n",
            "Epoch 57/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.5391\n",
            "Epoch 58/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 0.5233\n",
            "Epoch 59/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.5083\n",
            "Epoch 60/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 0.4920\n",
            "Epoch 61/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4769\n",
            "Epoch 62/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4620\n",
            "Epoch 63/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4478\n",
            "Epoch 64/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4345\n",
            "Epoch 65/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4213\n",
            "Epoch 66/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.4073\n",
            "Epoch 67/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3936\n",
            "Epoch 68/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3812\n",
            "Epoch 69/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3682\n",
            "Epoch 70/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3559\n",
            "Epoch 71/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3435\n",
            "Epoch 72/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3313\n",
            "Epoch 73/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3189\n",
            "Epoch 74/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.3081\n",
            "Epoch 75/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2977\n",
            "Epoch 76/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2867\n",
            "Epoch 77/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.2771\n",
            "Epoch 78/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2666\n",
            "Epoch 79/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2574\n",
            "Epoch 80/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2473\n",
            "Epoch 81/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2372\n",
            "Epoch 82/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2283\n",
            "Epoch 83/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2195\n",
            "Epoch 84/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2113\n",
            "Epoch 85/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.2030\n",
            "Epoch 86/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1949\n",
            "Epoch 87/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1875\n",
            "Epoch 88/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 0.1808\n",
            "Epoch 89/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1732\n",
            "Epoch 90/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.1664\n",
            "Epoch 91/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.1599\n",
            "Epoch 92/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1536\n",
            "Epoch 93/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1476\n",
            "Epoch 94/100\n",
            "42/42 [==============================] - 2s 48ms/step - loss: 0.1414\n",
            "Epoch 95/100\n",
            "42/42 [==============================] - 2s 50ms/step - loss: 0.1361\n",
            "Epoch 96/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1307\n",
            "Epoch 97/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1256\n",
            "Epoch 98/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1209\n",
            "Epoch 99/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1161\n",
            "Epoch 100/100\n",
            "42/42 [==============================] - 2s 49ms/step - loss: 0.1117\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f77a051dd10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим модель кодера, на входе далее будут закодированные вопросы, на выходе состояния state_h, state_c\n",
        "chat_encoderModel_1LSTM = Model(encoderInputs_t1, encoderStates_t1) \n",
        "\n",
        "decoderStateInput_h_1LSTM = Input(shape=(200 ,)) # Добавим входной слой для state_h\n",
        "decoderStateInput_c_1LSTM = Input(shape=(200 ,)) # Добавим входной слой для state_c\n",
        "decoderStatesInputs_1LSTM = [decoderStateInput_h_1LSTM, decoderStateInput_c_1LSTM] \n",
        "chat_decoderOutputs_1LSTM, chat_state_h_1LSTM, translate_state_c_1LSTM = decoderLSTM_t1(decoderEmbedding_t1, initial_state=decoderStatesInputs_1LSTM) \n",
        "chat_decoderStates_1LSTM = [chat_state_h_1LSTM, translate_state_c_1LSTM]            \n",
        "chat_decoderOutputs_1LSTM = decoderDense_t1(chat_decoderOutputs_1LSTM) \n",
        "chat_decoderModel_1LSTM = Model([decoderInputs_t1] + decoderStatesInputs_1LSTM, [chat_decoderOutputs_1LSTM] + chat_decoderStates_1LSTM)\n",
        "\n"
      ],
      "metadata": {
        "id": "1BCzeA7rB-Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strToTokens(sentence: str):      \n",
        "\n",
        "    ''' Функция для удаления пробелов перед знаками препинания\n",
        "\n",
        "        Args: фраза\n",
        "\n",
        "        Returns: список токенов\n",
        "    '''\n",
        "\n",
        "    # Почистим фразу\n",
        "    tmp_sent = my_replacer(sentence)  \n",
        "    \n",
        "    # Приведем предложение к нижнему регистру и разбирает на слова\n",
        "    words = tmp_sent.lower().split()  \n",
        "    #print(words)\n",
        "    # Создадим список для последовательности токенов/индексов\n",
        "    tokensList = list()               \n",
        "\n",
        "    # Для каждого слова в предложении\n",
        "    for word in words:\n",
        "        \n",
        "        try:\n",
        "            tokensList.append(tokenizer.word_index[word]) # Определяем токенайзером индекс и добавляем в список\n",
        "        except:\n",
        "            pass # Слова нет - просто игнорируем его\n",
        "\n",
        "    # Вернёт входную фразу в виде последовательности индексов\n",
        "    if tokensList:\n",
        "        return pad_sequences([tokensList], maxlen=maxLen_Questions , padding='post')\n",
        "\n",
        "    # Фраза из незнакомых слов - вернем None \n",
        "    return None"
      ],
      "metadata": {
        "id": "e8guIcyqB-cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "emptyTargetSeq = np.zeros((1, 1))\n",
        "\n",
        "\n",
        "emptyTargetSeq[0, 0] = 5\n",
        "\n",
        "emptyTargetSeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pROVCjKnAcKd",
        "outputId": "6a0d865c-473b-4b34-cb32-cbb5529be35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def ChatAnswer(Sentence):\n",
        "  qua  = strToTokens(Sentence)\n",
        "  if qua is None:                                      \n",
        "    return None\n",
        "  LastWord = np.zeros((1, 1))                    \n",
        "  LastWord[0, 0] = tokenizer.word_index['start'] \n",
        "  stopCondition = False                                \n",
        "  decodedTranslation = '' \n",
        "  statesValues = chat_encoderModel_1LSTM.predict(qua)                              \n",
        "\n",
        "  # пока не сработало стоп-условие\n",
        "  while not stopCondition:                             \n",
        "        # В модель декодера подадим пустую последовательность со словом 'start' и состояния\n",
        "        decOutputs , h , c = chat_decoderModel_1LSTM.predict([LastWord] + statesValues)\n",
        "        #print(np.shape(decOutputs),np.shape(h),np.shape(c))\n",
        "        # Получим индекс предсказанного слова.\n",
        "        #print(decOutputs[0, 0, :].shape[0])\n",
        "        #print(random.choices(np.array(range(decOutputs[0, 0, :].shape[0])), weights=decOutputs[0, 0, :]))\n",
        "        \n",
        "        sampledWordIndex = random.choices(np.array(range(decOutputs[0, 0, :].shape[0])), weights=decOutputs[0, 0, :])[0]\n",
        "        #print(sampledWordIndex)\n",
        "        # Создаем переменную для преобразованных на естественный язык слов\n",
        "        sampledWord = None                                 \n",
        "\n",
        "        # Переберем в цикле все индексы токенайзера\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "        \n",
        "          # Если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "          if (sampledWordIndex == index):\n",
        "            # Выбранное слово фиксируем в переменную sampledWord\n",
        "            sampledWord = word                       \n",
        "            if(sampledWord != 'end'):\n",
        "              # Слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "              decodedTranslation += ' {}'.format(word) \n",
        "            \n",
        "        \n",
        "        # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "        if sampledWord == 'end' or len(decodedTranslation.split()) > maxLen_Answers_SE:\n",
        "          stopCondition = True # Срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "        # Создаем пустой массив\n",
        "        LastWord = np.zeros((1, 1))       \n",
        "        \n",
        "        # Заносим в него индекс выбранного слова\n",
        "        LastWord[0, 0] = sampledWordIndex \n",
        "        \n",
        "        # Записываем состояния, обновленные декодером \n",
        "        statesValues = [h, c]   \n",
        "  return decodedTranslation"
      ],
      "metadata": {
        "id": "uGt7pXyeEiG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сам с собой чатится"
      ],
      "metadata": {
        "id": "fm0X8udVLyy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StartSentnce=Questions[100]\n",
        "print(StartSentnce)\n",
        "for i in range(0,200):\n",
        "  StartSentnce=ChatAnswer(StartSentnce)\n",
        "  print(StartSentnce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zaPt3CxEiP7",
        "outputId": "d9303d7f-5fde-49d6-b2ba-e88aaa8efb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ну где ты там?\n",
            " потому что видите.\n",
            " я тоже это чувствую.\n",
            " конечно.\n",
            " очередь.\n",
            " за же так нибудь ли я если едет было от тебе куклу.\n",
            " нет. я карандашей.\n",
            " да.\n",
            " странно.\n",
            " он приходил у меня невозможное.\n",
            " если я буду но но тоже с последний раз?\n",
            " с удовольствием.\n",
            " подавать горячее?\n",
            " да, пожалуйста.\n",
            " сейчас пожалуйста.\n",
            " зачем?\n",
            " чтобы к шести?\n",
            " да!\n",
            " тогда выйдите и вернитесь на свое место.\n",
            " да, сейчас!\n",
            " ну, мне минералочки.\n",
            " других надо, чтобы что очень...\n",
            " бога лет.\n",
            " и вон то? вроде же!\n",
            " хотим вам есть?\n",
            " только на подобрала, в того .\n",
            " какое?\n",
            " вот тот самое.\n",
            " да, вот немного.\n",
            " да... я хорошо бы вон\n",
            " и совсем помянем найдется!\n",
            " ну и нужно о нее два всего!\n",
            " да, ничего, есть?\n",
            " ну, есть.\n",
            " чугунок есть.\n",
            " повидать нет, я хочется?\n",
            " нет.\n",
            " разве здесь правда, не знаете?\n",
            " да, ладно...\n",
            " да, конечно, с тяжко в жива\n",
            " скорее к сядь.\n",
            " сегодня в порядке.\n",
            " тысяча есть.\n",
            " ее. пять до перешли\n",
            " да, можешь, мы никого нет.\n",
            " а вы на последний нет, поедем, мама.\n",
            " из работы.\n",
            " все, что же еще?\n",
            " николай.\n",
            " ну да. у меня и так куришь!\n",
            " теперь сейчас. только там, да не тебя бумажного дверь.\n",
            " ты замороженные.\n",
            " сама сидят.\n",
            " ух!\n",
            " значит, он не повторить? приехали было я.\n",
            " нет.\n",
            " где?\n",
            " вот было именно.\n",
            " да.\n",
            " как, уж вы обычно держите ее не влипал.\n",
            " к кому, только это водка.\n",
            " ну, в повезло чуток? киши.\n",
            " наличными.\n",
            " не первый ли ну, если ближе.\n",
            " она у там.\n",
            " а тебе справлюсь.\n",
            " все?\n",
            " прекрасно.\n",
            " несли. ладно, наверное отлично.\n",
            " это нам убрался. у нас даже есть не нее что ли?\n",
            " давай какая две признаками похоже, спасибо, спасибо, никому!\n",
            " приходил.\n",
            " тут консерватор. вы надо. деточка!\n",
            " когда нам чего, я меня не утра.\n",
            " вот!\n",
            " вот и пятый.\n",
            " да.\n",
            " а кого дурак.\n",
            " час не оба езды.\n",
            " вольно, влюбилась.\n",
            " один.\n",
            " кто здесь так мускулы нашего идиоты!\n",
            " кто все равно.\n",
            " здешний из вас десять.\n",
            " об полагать, все.\n",
            " ну вот он.\n",
            " да, конечно.\n",
            " телеграмма.\n",
            " о чем точно конечно.\n",
            " почему вы буду знаешь.\n",
            " у нас случайно место оказался телефон.\n",
            " прямо может, и мы меня не слышь,\n",
            " я думаю, я всегда.\n",
            " наверное. и вас?\n",
            " а себя для нас ассоль.\n",
            " а,\n",
            " думаю не бойся.\n",
            " отдай хватить.\n",
            " кто?\n",
            " неважно... я буду там плохо я?\n",
            " не письмо пойду да ладно.\n",
            " да да, а очень стой на да, дверь.\n",
            " она факт.\n",
            " сорок ложись думаете,\n",
            " перекрыт ты смогу мести? будем.\n",
            " смогу только хамить.\n",
            " голова к умерла.\n",
            " а вот раз вон,\n",
            " так в это прямо бога двадцать этой кореша.\n",
            " когда с вами боюсь.\n",
            " нисколько.\n",
            " так не ясно.\n",
            " второй не нравится.\n",
            " разрешите нет, на держишь исполнилось.\n",
            " все на ней.\n",
            " а что ты вчера болеет.\n",
            " по моему, трое лучше так.\n",
            " вы! бойся, зуди.\n",
            " я то чисто.\n",
            " святой они многому что? потому, нет,\n",
            " дождь уже выходит, спрашивали.\n",
            " немного.\n",
            " прекрасно, ну ка прочтите.\n",
            " не очень при смысле мысль.\n",
            " не буду.\n",
            " хочу!\n",
            " не где?\n",
            " на площади в больнице.\n",
            " ужасное. давай растворимый.\n",
            " валите едем\n",
            " сейчас нет тут домой, репетирует семьдесят балтика\n",
            " ну еще женаты, у меня три времени.\n",
            " она хочешь хотелось.\n",
            " успеешь менее.\n",
            " конечно.\n",
            " начнем. хотите чаю?\n",
            " с удовольствием...\n",
            " если или не с то, при салон.\n",
            " вот в порядке.\n",
            " да, кентавры!\n",
            " ну, и мы в да, раз шестого.\n",
            " еще не было.\n",
            " кажется, попробуйте.\n",
            " а нет, мадам, тебе водка.\n",
            " это я вас время.\n",
            " ты о шести?\n",
            " да! да! ну да, к так жилец?\n",
            " лично конечно.\n",
            " скорая по уpоки другая уже бросил успел.\n",
            " ка ну...\n",
            " не знаю не здесь, подслушиваете, сваливать...\n",
            " кто все равно.\n",
            " слышь, мужик.\n",
            " давайте чашечку когда потом мне.\n",
            " десять идемте вояка.\n",
            " то самый. что нибудь ясно?\n",
            " даже тебе четвертый.\n",
            " конечно! он взял нам одну один...\n",
            " конечно! а сказал отлично.\n",
            " он видел. вы никого не будете еще.\n",
            " нет, я палатке, ни рядом.\n",
            " конечно бы!\n",
            " да, сюда. да,\n",
            " ты знаешь чтобы скоро такова если...\n",
            " мне чего?\n",
            " ничего.\n",
            " что ты... вот же человека посмотрим веру?\n",
            " есть!\n",
            " выключить тогда?\n",
            " нет. и вижу.\n",
            " могу офицеров только медлить.\n",
            " давай.\n",
            " за моих ..знаю...\n",
            " ну не его пищу я?\n",
            " да. теперь чего вы, они они пожалуй, это...\n",
            " это нас. кто долгая .\n",
            " ах, это не смешно.\n",
            " все его при чем продолжать!\n",
            " все в порцию.\n",
            " завтра все взял.\n",
            " наверное, что то он, пять.\n",
            " одну никаких.\n",
            " сначала все нормально.\n",
            " чего мне при мессен, его есть.\n",
            " его люди неужели в пошли...\n",
            " этих не весь два море вопросов.\n",
            " нет.\n",
            " может, мои первое.\n",
            " пожалуй, тебе видел. царапина!.. на так...\n",
            " здесь из давно.\n",
            " зачем зря из мог насчет\n",
            " забудь.\n",
            " ну, так можно сюда...\n",
            " она моя родственница.\n",
            " да, сударь.\n",
            " я не исключено.\n",
            " сто нет.\n",
            " нет, они.., надо запрещено.\n",
            " будем рок ты не поможет.\n",
            " на две молодой сударь.\n",
            " есть. и с тобой у меня ну, и тарелочка.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Questions[0:10]"
      ],
      "metadata": {
        "id": "d2AhFb9zxpGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb72675-4481-4748-e137-25d7c7a4b12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Как вы можете быть таким уверенным ?',\n",
              " 'А что делать будем ?',\n",
              " 'Ждать .',\n",
              " 'Надеюсь , не до первой звезды ?',\n",
              " 'За что ?',\n",
              " 'Ты что нибудь понимаешь ?',\n",
              " 'Обсудим это завтра , ладно ?',\n",
              " 'Кто услышит ?',\n",
              " 'Что если ?',\n",
              " 'Ну я пойду ?']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iFBtlxoLRNT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С вниманием"
      ],
      "metadata": {
        "id": "5ahn-rfhRNwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(phrases): # Функция принимает содержимое словаря\n",
        "\n",
        "  # Разделяем пробелами слова и знаки препинания(\"А как насчет тебя? \" -> \"А как насчет тебя ? \") \n",
        "  phrases = re.sub(r\"([?.!,;:])\", r\" \\1 \", phrases) # r\" \\1 \" берёт значения 1й группы в скобках; обрамляем указанные символы пробелами\n",
        "\n",
        "  # Заменяем всё на пробелы, за исключением (a-zA-Zа-яёА-ЯЁ?.!,;:)\n",
        "  phrases = re.sub(r\"[^a-zA-Zа-яёА-ЯЁ?.!,;:]+\", \" \", phrases) \n",
        "  \n",
        "  # Получаем строку без случайных лишних пробелов в конце фраз(rstrip удаляет с конца строки)\n",
        "  phrases = phrases.rstrip().strip()      \n",
        "\n",
        "  # Для нашей модели обозначим тегами начало и конец предложения  \n",
        "  phrases = '<start> ' + phrases + ' <end>' \n",
        "\n",
        "  # Функция возвращает предобработанные фразы\n",
        "  return phrases "
      ],
      "metadata": {
        "id": "kWDElaNBxqwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Разделяем пробелами слова и знаки препинания(\"А как насчет тебя? \" -> \"А как насчет тебя ? \") \n",
        "  #Questions = [re.sub(r\"([?.!,;:])\", r\" \\1 \", s) for s in Questions] # r\" \\1 \" берёт значения 1й группы в скобках; обрамляем указанные символы пробелами\n",
        "  #Answers = [re.sub(r\"([?.!,;:])\", r\" \\1 \", s) for s in Answers] # r\" \\1 \" берёт значения 1й группы в скобках; обрамляем указанные символы пробелами\n",
        "\n",
        "  # Заменяем всё на пробелы, за исключением (a-zA-Zа-яёА-ЯЁ?.!,;:)\n",
        "  #Questions = [re.sub(r\"[^a-zA-Zа-яёА-ЯЁ?.!,;:]+\", \" \", s) for s in Questions]\n",
        "  #Answers = [re.sub(r\"[^a-zA-Zа-яёА-ЯЁ?.!,;:]+\", \" \", s) for s in Answers]\n",
        "  #Answers = re.sub(r\"[^a-zA-Zа-яёА-ЯЁ?.!,;:]+\", \" \", Answers) \n",
        "  \n",
        "  # Получаем строку без случайных лишних пробелов в конце фраз(rstrip удаляет с конца строки)\n",
        "  #Questions = [s.rstrip().strip()  for s in Questions]#Questions.rstrip().strip()      \n",
        "  #Answers = [s.rstrip().strip()  for s in Answers]#Answers.rstrip().strip()      \n",
        "  \n",
        "  # Для нашей модели обозначим тегами начало и конец предложения  \n",
        "  #Questions_SE = ['<START> ' + s + ' <END>' for s in Questions]\n",
        "  #Answers_SE = ['<START> ' + s + ' <END>' for s in Answers]\n",
        "  Questions_SE = [preprocess_sentence(s) for s in Questions]\n",
        "  Answers_SE = [preprocess_sentence(s) for s in Answers]\n",
        "  \n"
      ],
      "metadata": {
        "id": "I2WUa-xzxq3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5xw1cPZqu5d"
      },
      "source": [
        "# Создадим мини-функцию, возвращающую максимальную длину тензора\n",
        "def max_length(tensor): # Функция принимает на вход тензор(фразы в виде последовательности индексов)\n",
        "\n",
        "  # Вернем значение максимальной длины его элемента \n",
        "  return max(len(t) for t in tensor) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(filters='')               # Вызываем класс Токенизатор, просим его не удалять символы, которые он удаляет по умолчанию\n",
        "tokenizer.fit_on_texts(Questions_SE + Answers_SE)                # \"скармливаем\" ему тексты для обработки и сборки словаря частотности\n",
        "Answers_tensor = tokenizer.texts_to_sequences(Answers_SE) # Формируем тензоры и токенизатор для английского языка\n",
        "Questions_tensor = tokenizer.texts_to_sequences(Questions_SE)\n",
        "Answers_tensor = pad_sequences(Answers_tensor, padding='post')\n",
        "Questions_tensor = pad_sequences(Questions_tensor, padding='post')"
      ],
      "metadata": {
        "id": "ZcNW0-dexpMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Questions_SE[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l9wYEzEnkLxQ",
        "outputId": "6eeda544-8689-457c-d418-b60aed5ade56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> А что делать будем ? <end>'"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_Q = max_length(Questions_tensor)\n",
        "max_length_A = max_length(Answers_tensor)\n",
        "max_length=max(max_length_Q,max_length_A)\n",
        "# Создаем тренировочную и тестовую выборки по формуле 95/5\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(Questions_tensor, Answers_tensor, test_size=0.05)"
      ],
      "metadata": {
        "id": "xSrsuBvZxpTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализируем собранные данные\n",
        "\n",
        "def convert(language_tokenizer,  # Токенайзер\n",
        "            tensor):             # Список индексов слов\n",
        "            \n",
        "  #  Цикл по токенам во фразе\n",
        "  for t in tensor:  \n",
        "    if t!=0:                                                        # Если токен не 0. Т.е. не мусор в конце фразы\n",
        "      print (\"%d ----> %s\" % (t, tokenizer.index_word[t])) # Выводи токен и соответствующее слово\n",
        "\n"
      ],
      "metadata": {
        "id": "Em7thatqxpbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Questions\")   \n",
        "convert(tokenizer, input_tensor_train[0])           # Выведем нулевую пару из вопросов\n",
        "print ()    \n",
        "\n",
        "print (\"Answers\")\n",
        "convert(tokenizer, target_tensor_train[0])         # Выведем нулевую пару из ответов\n",
        "print ()   \n",
        "                                                      \n",
        "print(\"Questions тренировочная: \" , len(input_tensor_train), \"фраз; \", \"Answers тренировочная: \", len(target_tensor_train), \"фраз\")# Выведем статистику по обучающей выборке\n",
        "print(\"Questions тестовая: \", len(input_tensor_val), \"фраз; \", \"Answers тестовая: \", len(target_tensor_val), \"фраз\")               # Выведем статистику по тестовой выборке"
      ],
      "metadata": {
        "id": "crgwcRBKxphY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db64596-2b50-4817-90fa-7acc554c7be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions\n",
            "1 ----> <start>\n",
            "47 ----> может\n",
            "89 ----> быть\n",
            "5 ----> ,\n",
            "6 ----> что\n",
            "74 ----> нибудь\n",
            "40 ----> еще\n",
            "4 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Answers\n",
            "1 ----> <start>\n",
            "150 ----> больше\n",
            "45 ----> ничего\n",
            "7 ----> не\n",
            "62 ----> надо\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Questions тренировочная:  10088 фраз;  Answers тренировочная:  10088 фраз\n",
            "Questions тестовая:  531 фраз;  Answers тестовая:  531 фраз\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим постоянные \n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)                     # Укажем что случайно сэмплировать будем по всей длине обучающейся выборки\n",
        "BATCH_SIZE = 256                                          # Указываем размер батча\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE     # Укажем количество шагов в одной эпохе\n",
        "embedding_dim = 2048                                       # Размерность эмбеддинга, векторного пространства\n",
        "units = 1024                                              # Задаем размер слоя(количество нейронов в слое) "
      ],
      "metadata": {
        "id": "KxxWEZw0G-s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем размер словаря\n",
        "vocab_size = len(tokenizer.word_index)+1 \n",
        "\n",
        "# Создаём датасет из массивов Numpy(рус и анг тренировочные фразы) со случайной подачей тренировочных сэмплов в процессе обучения\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Передаем в датасет размер батча и указываем, что если в тренировке последний батч окажется неполным, то опустим его\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "SQgH6qyTG-wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Посмотрим на форму примеров полученных батчей\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzQUa4vG-zz",
        "outputId": "fc4b9ad3-ce8a-469e-bac5-af87b0800e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([256, 19]), TensorShape([256, 22]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTX4UFW8JeGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJwqiGRCDCu1"
      },
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "  # Конструктор класса \n",
        "  def __init__(self, \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размер пространсва эмбеддинга\n",
        "               enc_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "\n",
        "    super(Encoder, self).__init__()                                   # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                                          # Атрибут возвращает размер батча\n",
        "    self.enc_units = enc_units                                        # Атрибут возвращает размер слоя в кодировщике\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)             # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и с dim=256\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки\n",
        "    self.gru = GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # Метод принимает входную фразу и начальное состояние\n",
        "  def call(self, \n",
        "           x,       # Входная фраза\n",
        "           hidden): # Начальное энкодера\n",
        "    x = self.embedding(x) # входящие тензоры преобразовываются в эмбеддинг\n",
        "    output, state = self.gru(x, initial_state = hidden) #затем пропускаются через GRU и получаем выход + новое состояние\n",
        "\n",
        "    # Выход сети GRU и состояние на выходе\n",
        "    return output, state \n",
        "\n",
        "  # Создаем метод инициализации состояний на скрытых слоях\n",
        "  def initialize_hidden_state(self):\n",
        "\n",
        "    # Вернем тензор из нулей размер батча на размер слоя, итсполбьзуем как начальное состояние энкодера\n",
        "    return tf.zeros((self.batch_sz, self.enc_units)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR4bQIKDFXF"
      },
      "source": [
        "# Создадим модель кодировщика по уже заданным параметрам \n",
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXH74Z-B_Mmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f8256c-c1d3-4ba7-ad5a-7bbc3b238938"
      },
      "source": [
        "# Подадим в качестве примера какой-то сэмпл(Тензор[64, 12]) на вход Encoder'у и визуализируем, что получим\n",
        "sample_hidden = encoder.initialize_hidden_state() #инициализируем начальное скрытое состояние\n",
        "\n",
        "# Даем Encoder'у сэмпл и начальное состояние, и получим выход из сети GRU и состояние на выходе (вызывается метод call класса Encoder)\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Размеры выхода из кодировщика: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Размеры скрытого состояния: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размеры выхода из кодировщика: (batch size, sequence length, units) (256, 19, 1024)\n",
            "Размеры скрытого состояния: (batch size, units) (256, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDTK8eqDHu0"
      },
      "source": [
        "class BahdanauAttention(Model): # Название класса именем создателя механизма Дмитрия Богданова(Bahdanau)\n",
        "\n",
        "  # Создаем конструктор класса\n",
        "  def __init__(self, \n",
        "               units):                        # Число нейронов \n",
        "\n",
        "    super(BahdanauAttention, self).__init__() # Даем возможность использовать и исполнять методы класса-родителя в классе потомке\n",
        "    self.W1 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.W2 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.V =  Dense(1)                        # Создаем Dense с числом нейронов =1\n",
        "\n",
        "  # Метод принимает состояние и выход энкодера ----------------------------------\n",
        "  \n",
        "  def call(self, \n",
        "           hidden_state, # Состояние энкодера\n",
        "           values):      # Выход энкодера\n",
        "    # Форма состояния на скрытом слое (batch_size, hidden size)\n",
        "    # Форму состояния на каждом такте увеличим до (batch_size, 1, hidden size)\n",
        "    # Добавляем это для того, чтобы получить оценку\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
        "    #print(hidden_state.shape)\n",
        "    #print(values.shape)\n",
        "    #print(hidden_with_time_axis.shape)\n",
        "    #print(values[1,11,5])\n",
        "    #print(hidden_with_time_axis[1,0,5])\n",
        "    # Форма оценки score (размер батча, макс.длина слов на входе, 1), однёрка в конце, чтобы применить self.V\n",
        "    # До применения self.V оценка была бы (размер батча, макс.длина слов на входе, количество нейронов в слое)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    #print(self.W1(values).shape)\n",
        "    #print(self.W2(hidden_with_time_axis).shape)\n",
        "    #print((self.W1(values) + self.W2(hidden_with_time_axis)).shape)\n",
        "    #print(score.shape)\n",
        "    \n",
        "    # К полученной оценке применим Софтмакс, который покажет вероятность полезности от 0 до 1 для каждого слова в фразе для декодера\n",
        "    # Форма оценки score - (размер батча, макс.длина слов на входе, 1); Софтмакс применяем к оси \"макс.длина слов\"\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    #print(attention_weights.shape)\n",
        "    \n",
        "    # Построим вектор контекста \n",
        "    context_vector = attention_weights * values # Веса внимания перемножим со значениями(выхода из кодировщика)\n",
        "    #print(context_vector.shape)\n",
        "    # Сумму также применяем по оси \"макс.длина слов на входе\"\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) # Размеры вектора контекста после суммирования будут (размер батча, размер слоя)\n",
        "    #print(context_vector.shape)\n",
        "\n",
        "    # Возвращает вектор контекста и веса внимания\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUv-DSsDDKGl",
        "outputId": "d9f64fcf-3116-4ff0-ca44-b438d6fb4415"
      },
      "source": [
        "# Проверим, как работает слой\n",
        "attention_layer = BahdanauAttention(10)\n",
        "\n",
        "# Подадим на вход слою внимания выход из Encodera и его состояние, и получим значение и веса внимания\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Размеры значения внимания: (размер батча, размер слоя) {}\".format(attention_result.shape))\n",
        "print(\"Размеры весов внимания: (размер батча, длина последовательности, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 1024)\n",
            "(256, 19, 1024)\n",
            "(256, 1, 1024)\n",
            "tf.Tensor(-0.029745221, shape=(), dtype=float32)\n",
            "tf.Tensor(-0.043278717, shape=(), dtype=float32)\n",
            "(256, 19, 10)\n",
            "(256, 1, 10)\n",
            "(256, 19, 10)\n",
            "(256, 19, 1)\n",
            "(256, 19, 1)\n",
            "(256, 19, 1024)\n",
            "(256, 1024)\n",
            "Размеры значения внимания: (размер батча, размер слоя) (256, 1024)\n",
            "Размеры весов внимания: (размер батча, длина последовательности, 1) (256, 19, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHTEgDoDNBC"
      },
      "source": [
        "class Decoder(Model):\n",
        "\n",
        "  # Создадим конструктор класса\n",
        "  def __init__(self,   \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размерность пространства эмбеддинга\n",
        "               dec_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "    super(Decoder, self).__init__()                       # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                              # Атрибут возвращает размер батча\n",
        "    self.dec_units = dec_units                            # Атрибут возвращает размер слоя в декодере(кол-во нейронов)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim) # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и (dim=256) на выходе\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки    \n",
        "    self.gru = GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = Dense(vocab_size) # Атрибут вызовет полносвязный слой с размером словаря\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units) #атрибут подключит механизм внимания, описанный ранее\n",
        "\n",
        "\n",
        "  def call(self, \n",
        "           x,           # Начальный токен\n",
        "           hidden,      # Состояние  энкодера\n",
        "           enc_output): # Выход энкодера\n",
        "\n",
        "    # Enc_output размеры (batch_size, max_length, hidden_size - размер батча, макс.длина фраз, разм.скр.слоя)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    #print('Decoder')\n",
        "    #print(context_vector.shape)\n",
        "    #print(attention_weights.shape)\n",
        "    \n",
        "    # Входящий тензор слова пропускаем через эмбеддинг (получаем размеры batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    #print(x.shape)\n",
        "    \n",
        "    # Дальше конкатенируем с вектором контекста (получаем размеры batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    #print(tf.expand_dims(context_vector, 1).shape)\n",
        "    #print(x.shape)\n",
        "    \n",
        "    # Сконкатенированный вектор передаем  в GRU и получаем выход с декодера и состояние\n",
        "    output, state = self.gru(x)\n",
        "    #print(output.shape)\n",
        "    #print(state.shape)\n",
        "\n",
        "    # Output размеры (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    #print(output.shape)\n",
        "\n",
        "    # Пропускаем через полносвязный слой\n",
        "    x = self.fc(output) #output размеры (batch_size, vocab)\n",
        "    #print(x.shape)\n",
        "\n",
        "    # Вернем выходную фразу, вектор состояния, веса внимания\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b1yEPu3DPqp",
        "outputId": "9cc52147-d242-494b-80df-c01ba120d383"
      },
      "source": [
        "# Проверим работу декодера, подав на вход случайный массив с нужной размерностью\n",
        "# Создали декодер с параметрами(размер анг.словаря, размерность эмбеддинга, кол-во нейронов, размер батча)\n",
        "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Подаём на вход случайный массив с нужной размерностью, состояние и выход с кодировщика\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((256, 1)), sample_hidden, sample_output)\n",
        "print ('Размер выхода с декодера: (размер батча, размер словаря) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер выхода с декодера: (размер батча, размер словаря) (256, 11397)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h7uiC6LDVRK"
      },
      "source": [
        "# Выбираем оптимайзер Adam\n",
        "optimizer = Adam()#learning_rate=0.1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YVxy08qDR-5"
      },
      "source": [
        "# Используем SparseCategoricalCrossentropy, к-я может работать с некатегориальными лейблами\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # Выбираем функцию потерь\n",
        "\n",
        "def loss_function(real, pred):                       # Запишем функцию потерь, на вход подаем фактический и предсказанный результат\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # Найдем маску, которая уберет нулевые значения индексов в конце фразы\n",
        "  loss_ = loss_object(real, pred)                    # Фактические и предсказанные результаты передаем в SparseCategoricalCrossentropy и получаем ошибку\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)            # Согласуем тип маски с типом потерь\n",
        "  loss_ *= mask                                      # Накидываем \"маску\" которая оставит для работы ненулевые значения\n",
        "  \n",
        "  # Вернем reduce_mean - среднее любого выбранного тензора\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHr21E05q0NN"
      },
      "source": [
        "# Сохраняем процесс обучения модели чекпоинтами тензорфлоу\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'                                               # Даем ссылку на директорию\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")                                # Добавляем префикс \"ckpt\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder) # Сохраняем состояния/показатели оптимизатора и моделей"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCNvA66Jq7nE"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp,         # Входная фраза\n",
        "               targ,        # Точный перевод\n",
        "               enc_hidden): # Состояния энкодера\n",
        "\n",
        "  # Создаем переменную, в которую будем записывать ошибку\n",
        "  loss = 0                             \n",
        "\n",
        "  # Все операции по вычислению градиента записываются на ленту(tape) и мы получаем к ним доступ\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Передаем тензор и начальное состояние в кодировщик и получим выход и состояние на выходе\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # Передадим это состояние декодеру\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # Передаем в качестве входа в декодер индекс токена \"<start>\"\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Техника \"Teacher forcing\" - подаем предыдущее выходное слово на вход следущего в декодере. Targ.shape[64, 9]\n",
        "\n",
        "    for t in range(1, targ.shape[1]): #для каждого слова из английской фразы\n",
        "\n",
        "      # Передаем в обработку декодеру начальный токен, состояние на выходе из кодера, и выход из кодера\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) # Получаем от декодера предсказание и обновленное состояние\n",
        "\n",
        "      # Обновляем ошибку для текущих предсказаний\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Используем \"Teacher forcing\"\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # Получаем ошибку на батче . Targ.shape[64, 9]. Делим на 9\n",
        "  batch_loss = (loss / int(targ.shape[1])) \n",
        "\n",
        "  # Создаем список переменных, для которых TensorFlow будет вычислять градиенты\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables # создаем переменные, для которых TensorFlow будет вычислять градиенты\n",
        "\n",
        "  # Отслеживаем градиент\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Корректируем веса\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Функция обучения вернет ошибку на батче\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Tpy6Cxq-V3",
        "outputId": "2bde5e54-bf7e-41bd-dabe-28fc7e9436c7"
      },
      "source": [
        "EPOCHS = 50 # устанавливаем количество эпох\n",
        "\n",
        "for epoch in range(EPOCHS): # Цикл по каждой эпохе\n",
        "  start = time.time() # Запомним время начала эпохи\n",
        "\n",
        "  progbar = tf.keras.utils.Progbar(target=steps_per_epoch, stateful_metrics=[\n",
        "                                     'batch_loss','accuracy'], unit_name='batch')        # Создадим индикатор прогресс обучения\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state() # Задаем начальное состояние на скрытом слое encodera \n",
        "  total_loss = 0                                 # Начальное значение итоговой ошибки\n",
        "\n",
        "  # Для батча, входного и выходного тензора на каждом шаге эпохи\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden) # Передадим в функцию тензоры и состояние в кодировщике, обучим и получим ошибку на батче\n",
        "    total_loss += batch_loss                       # Добавим ее в итоговую ошибку\n",
        "    progbar.update(                                # Обновим состояние индикатора обучения\n",
        "            batch + 1, values=[('batch_loss', batch_loss)])\n",
        "\n",
        "\n",
        "  # Каждые 10 эпох будем сохранять чекпоинты\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  # Выведем показатели после каждой эпохи\n",
        "  print('Эпоха {} Ошибка {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch)) # Выведем номер эпохи и потери\n",
        "  print('Время на 1 эпоху {} сек'.format(round(time.time() - start), 1))          # Выведем длительность обучения этой эпохи"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 77s 1s/batch - batch_loss: 1.1065\n",
            "Эпоха 1 Ошибка 1.3449\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.9152\n",
            "Эпоха 2 Ошибка 0.9650\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.8060\n",
            "Эпоха 3 Ошибка 0.7979\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.6762\n",
            "Эпоха 4 Ошибка 0.6538\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.5668\n",
            "Эпоха 5 Ошибка 0.5273\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.4161\n",
            "Эпоха 6 Ошибка 0.4194\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.3199\n",
            "Эпоха 7 Ошибка 0.3311\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.2655\n",
            "Эпоха 8 Ошибка 0.2628\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.2302\n",
            "Эпоха 9 Ошибка 0.2098\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.1720\n",
            "Эпоха 10 Ошибка 0.1695\n",
            "Время на 1 эпоху 85 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.1388\n",
            "Эпоха 11 Ошибка 0.1381\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.1210\n",
            "Эпоха 12 Ошибка 0.1132\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0993\n",
            "Эпоха 13 Ошибка 0.0946\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0809\n",
            "Эпоха 14 Ошибка 0.0808\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0790\n",
            "Эпоха 15 Ошибка 0.0711\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0664\n",
            "Эпоха 16 Ошибка 0.0645\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0642\n",
            "Эпоха 17 Ошибка 0.0593\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0607\n",
            "Эпоха 18 Ошибка 0.0583\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0550\n",
            "Эпоха 19 Ошибка 0.0539\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0652\n",
            "Эпоха 20 Ошибка 0.0502\n",
            "Время на 1 эпоху 85 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0547\n",
            "Эпоха 21 Ошибка 0.0467\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0576\n",
            "Эпоха 22 Ошибка 0.0507\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0445\n",
            "Эпоха 23 Ошибка 0.0473\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0427\n",
            "Эпоха 24 Ошибка 0.0436\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0349\n",
            "Эпоха 25 Ошибка 0.0420\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0475\n",
            "Эпоха 26 Ошибка 0.0424\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0510\n",
            "Эпоха 27 Ошибка 0.0411\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0447\n",
            "Эпоха 28 Ошибка 0.0389\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0499\n",
            "Эпоха 29 Ошибка 0.0395\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0436\n",
            "Эпоха 30 Ошибка 0.0371\n",
            "Время на 1 эпоху 85 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0450\n",
            "Эпоха 31 Ошибка 0.0359\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0386\n",
            "Эпоха 32 Ошибка 0.0344\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0541\n",
            "Эпоха 33 Ошибка 0.0336\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0375\n",
            "Эпоха 34 Ошибка 0.0347\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0396\n",
            "Эпоха 35 Ошибка 0.0343\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0452\n",
            "Эпоха 36 Ошибка 0.0336\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0384\n",
            "Эпоха 37 Ошибка 0.0326\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0361\n",
            "Эпоха 38 Ошибка 0.0321\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0323\n",
            "Эпоха 39 Ошибка 0.0313\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0309\n",
            "Эпоха 40 Ошибка 0.0305\n",
            "Время на 1 эпоху 85 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0334\n",
            "Эпоха 41 Ошибка 0.0301\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0297\n",
            "Эпоха 42 Ошибка 0.0296\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0404\n",
            "Эпоха 43 Ошибка 0.0296\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0320\n",
            "Эпоха 44 Ошибка 0.0290\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0290\n",
            "Эпоха 45 Ошибка 0.0289\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0253\n",
            "Эпоха 46 Ошибка 0.0285\n",
            "Время на 1 эпоху 82 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0328\n",
            "Эпоха 47 Ошибка 0.0278\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0296\n",
            "Эпоха 48 Ошибка 0.0276\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 46s 1s/batch - batch_loss: 0.0291\n",
            "Эпоха 49 Ошибка 0.0274\n",
            "Время на 1 эпоху 46 сек\n",
            "39/39 [==============================] - 45s 1s/batch - batch_loss: 0.0348\n",
            "Эпоха 50 Ошибка 0.0269\n",
            "Время на 1 эпоху 49 сек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index['<start>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqZ9xtEPh4GI",
        "outputId": "ee397a59-2dff-4e7c-80aa-1da60d80c470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import random\n",
        "def evaluate(sentence):\n",
        "\n",
        "    # Создаем начальные настройки графика внимания\n",
        "    attention_plot = np.zeros((max_length, max_length)) \n",
        "    #print(sentence)\n",
        "    # Предобрабатываем предложение\n",
        "    sentence = preprocess_sentence(sentence).lower()\n",
        "    #print(sentence)\n",
        "    \n",
        "    inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]   # Преобразовываем в послед-ть индексов\n",
        "    inputs = pad_sequences([inputs], maxlen=max_length, padding='post')        # Делаем паддинг\n",
        "    inputs = tf.convert_to_tensor(inputs)                                          # Конвертируем в тф тензор\n",
        "\n",
        "    result = ''                                                                    # Сюда запишем результат\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]                                                # Задаем начальное состояние\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)                                  # Передаем его и входной тензор и получаем выход с кодера и состояние\n",
        "\n",
        "    dec_hidden = enc_hidden                                                        # Состояние кодера передаем в декодер\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0) # Передаем на вход декодеру <start> в виде индекса\n",
        "\n",
        "    for t in range(max_length):                                               # Идем по макс.длине фраз выходного языка(анг)\n",
        "        # Прогоняем через декодер входящий тензор, состояние с выхода кодера, выход с кодера\n",
        "        # Получаем результат предсказания, обновленное состояние, и веса внимания\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # Сохраняем веса внимания для графика\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        # Аргмаксом вытаскиваем предсказанное слово\n",
        "        predictions_min=np.amin(predictions[0])\n",
        "        predictions_max=np.amax(predictions[0])\n",
        "        #print(predictions_min,predictions_max)\n",
        "        #predictions_w=(np.array(predictions[0])-predictions_min)/(predictions_max-predictions_min)\n",
        "        #print(predictions_w.shape,np.std(predictions_w))\n",
        "        predictions_w=np.exp(predictions[0]) / np.sum(np.exp(predictions[0]), axis=0)\n",
        "        #print(predictions_w.shape,np.std(predictions_w))\n",
        "        #predictions_w=np.exp(predictions_w) / np.sum(np.exp(predictions_w), axis=0)\n",
        "        #predictions_w=np.exp(predictions_w) / np.sum(np.exp(predictions_w), axis=0)\n",
        "        #predictions_w=np.exp(predictions_w) / np.sum(np.exp(predictions_w), axis=0)\n",
        "        #print(predictions_w.shape,np.std(predictions_w))\n",
        "        #print(predictions_w[:10])\n",
        "        #predicted_id = random.choices(np.array(range(predictions_w.shape[0])), weights=predictions_w)\n",
        "        predicted_id = [np.random.choice(np.arange(predictions_w.shape[0]), p=predictions_w)]\n",
        "        #print(tf.argmax(predictions[0]).numpy(),tf.argmax(predictions_w).numpy(),predicted_id[0],np.amax(predictions_w),predictions_w[predicted_id[0]])\n",
        "        #print(predicted_id[0])\n",
        "        #print(predictions_w[predicted_id[0]])\n",
        "        predicted_id=predicted_id[0]\n",
        "        #print(predicted_id[0],predicted_id[1],predicted_id[2])\n",
        "        #predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # Результат конвертируем из индекса в слово и сохраняем в result = ''\n",
        "        result += tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        # Если предсказанное слово - <end>, то останавливаемся, возвращаем результаты, выводим на графике\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # Педсказанное значение подается обратно в модель\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # Вернем перевод, входную фразу и веса внимания\n",
        "    return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "E__WVdcHJeJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDPV82ybJeM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It8ZVMbHrFYj"
      },
      "source": [
        "def plot_attention(attention,           # Веса внимания\n",
        "                   sentence,            # Исходная фраза\n",
        "                   predicted_sentence): # Предсказаные перевод\n",
        "  \n",
        "    fig = plt.figure(figsize=(10,10))                                   # Зададим размер \n",
        "    ax = fig.add_subplot(1, 1, 1)                                       # Добавим 1 картинку\n",
        "    ax.matshow(attention, cmap='viridis')                               # Нарисуем 2d матрицу\n",
        "    fontdict = {'fontsize': 14}                                         # Зададим размер надписей\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90) # Добавим надпись по горизонтальной оси\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)    # Добавим надпись по вертикальной оси\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))               # Зададим форматирование делений на осях графиков\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))               # Зададим форматирование делений на осях графиков\n",
        "    plt.show()                                                          # Отрисуем изображение"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8G96tWrHmn"
      },
      "source": [
        "def translate(sentence): # Функция принимает предложение и выводит результат с визуализацией\n",
        "    result, sentence, attention_plot = evaluate(sentence)  # Отдадим фразу. Получим перевод, входную фразу,  веса внимания\n",
        "\n",
        "    print('Входящая фраза: %s' % (sentence))          # Выведем входную фразу \n",
        "    print('Предсказанный перевод: {}'.format(result)) # Выведем полученный перевод\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))] # Возьмем весы внимания, только для слов во фразах. Хвосты не смотрим\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))              # Выведем веса внимания"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ChatAnswer(sentence): # Функция принимает предложение и выводит результат с визуализацией\n",
        "    result, sentence, attention_plot = evaluate(sentence)  # Отдадим фразу. Получим перевод, входную фразу,  веса внимания\n",
        "    return result"
      ],
      "metadata": {
        "id": "z7VUrTLs1hiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vsbPAP5rJle",
        "outputId": "48a8ae18-4958-4112-9603-eff1bc0cf90d"
      },
      "source": [
        "# Воспроизведём последний сохранённый чекпоинт\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3a9fb37910>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Карты распределения внимания"
      ],
      "metadata": {
        "id": "YGlCRnl-1WoI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "bBi0SqkJrMeK",
        "outputId": "435c1a91-d8b2-4fa1-8755-333a0a300e43"
      },
      "source": [
        "translate('давайте дружить')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> давайте дружить <end>\n",
            "Предсказанный перевод: так . <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "X2VVb9LHrOv8",
        "outputId": "5165100c-1320-4ae6-a1f8-e72847f0984d"
      },
      "source": [
        "translate('у тебя всё хорошо')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> у тебя всё хорошо <end>\n",
            "Предсказанный перевод: я тоже есть десятицентовик . а тебе раз ! <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "uWC8-1DDrPQ1",
        "outputId": "dd14369f-849a-4fa4-8326-c721bd3c0b94"
      },
      "source": [
        "translate('у тебя всё хорошо?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входящая фраза: <start> у тебя всё хорошо ? <end>\n",
            "Предсказанный перевод: я тоже есть десятицентовик . <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сам с собой."
      ],
      "metadata": {
        "id": "ulQ9Y3DrJCcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StartSentnce=Questions[100]\n",
        "print(StartSentnce)\n",
        "for i in range(0,200):\n",
        "  StartSentnce=Answer(StartSentnce)\n",
        "  StartSentnce=StartSentnce.replace('<end>', '')\n",
        "  print(StartSentnce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mabVssIJePx",
        "outputId": "77b98b0d-3b46-469d-f7ac-a3c1c399f061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ну где ты там ?\n",
            "здесь , здесь .  \n",
            "кто ?  \n",
            "неважно .  \n",
            "правда , что ж нет .  \n",
            "что же не узнаю ?  \n",
            "днём .  \n",
            "а не бросите .  \n",
            "а что ?  \n",
            "ничего .  \n",
            "а идти ты можешь ?  \n",
            "да , здесь .  \n",
            "а !  \n",
            "а то выйдите и вас .  \n",
            "да что ?  \n",
            "да .  \n",
            "ну ладно , в другое место перешел .  \n",
            "куда ?  \n",
            "опять в карцер .  \n",
            "я его ж говорил тебе завсегда .  \n",
            "что это ?  \n",
            "сейчас разберемся .  \n",
            "какого такой ?  \n",
            "я чувствую , вы их отдам .  \n",
            "благодарю знаешь .  \n",
            "ты ?  \n",
            "я .  \n",
            "не понял . . .  \n",
            "а мы обе романовы .  \n",
            "сестры , что ль ?  \n",
            "однофамилицы .  \n",
            "паша , разберись .  \n",
            "а это не хочет ?  \n",
            "ты об этом делать то немного живы .  \n",
            "входите или завязали ?  \n",
            "и не развязывал .  \n",
            "вообще ?  \n",
            "вообще . дело в том .  \n",
            "а вы не погорячились ?  \n",
            "нет , только .  \n",
            "хочу к нам ?  \n",
            "от если будет .  \n",
            "от меня .  \n",
            "а надо ?  \n",
            "может , и надо .  \n",
            "ладно , вы ?  \n",
            "думаю , они .  \n",
            "это уже . . .  \n",
            "что , плеткой били ?  \n",
            "это скорее .  \n",
            "что это хорошо ?  \n",
            "у нас !  \n",
            "у кого ?  \n",
            "нет , забыла . . .  \n",
            "надо же .  \n",
            "когда вернешься ?  \n",
            "не знаю .  \n",
            "куда же ты поедешь ?  \n",
            "попробую ее поискать .  \n",
            "а ?  \n",
            "так , ничего .  \n",
            "а как будто лучше ?  \n",
            "так точно .  \n",
            "гм гм , вы не могли бы мне показать это окно ?  \n",
            "разумеется .  \n",
            "разумеется нет !  \n",
            "дао просто спал ?  \n",
            "да вот .  \n",
            "а чего ?  \n",
            "а то .  \n",
            "а как собака ?  \n",
            "что не туда ?  \n",
            "я уже .  \n",
            "что ?  \n",
            "сценарий принесли ?  \n",
            "а вы ее !  \n",
            "а я . . мы с ним .  \n",
            "а потом я его ?  \n",
            "все ж сам ничего .  \n",
            "конечно .  \n",
            "начнем . хотите чаю ?  \n",
            "с удовольствием . . .  \n",
            "подавать горячее ?  \n",
            "да , пожалуйста .  \n",
            "а вон там ли ?  \n",
            "а !  \n",
            "а , помнишь , а сколько она .  \n",
            "непременно !  \n",
            "убирайтесь из моего двора .  \n",
            "мужик , отвали !  \n",
            "мужики , не делайте этого , пожалеете .  \n",
            "отвали !  \n",
            "мужики , не стесняйся .  \n",
            "как будто .  \n",
            "а кого ты как подошел .  \n",
            "оставайтесь здесь , если раздеться тобой ?  \n",
            "сам не то .  \n",
            "у вас .  \n",
            "а ты ?  \n",
            "упала !  \n",
            "татуировки ?  \n",
            "переводные картинки .  \n",
            "но голос .  \n",
            "господи .  \n",
            "и поете подобную дрянь !  \n",
            "милая .  \n",
            "вы это прошептал это зажигание , вы не берет ?  \n",
            "да .  \n",
            "такое практикуется ?  \n",
            "он практиковал .  \n",
            "а это ?  \n",
            "я вернусь за ними .  \n",
            "я что , что ли ?  \n",
            "ты мне ?  \n",
            "нет .  \n",
            "а сюда , сколько хотите .  \n",
            "а , как хоть милиция .  \n",
            "кому меня куда ж сейчас уберу и куда такие зеленые деваться ?  \n",
            "а если хочешь .  \n",
            "и что ?  \n",
            "мы будем очень благодарны вам за участие .  \n",
            "а это такая ?  \n",
            "нет .  \n",
            "зачем тогда ?  \n",
            "в недавно , в самое , вы побудьте в квартиру .  \n",
            "уверен .  \n",
            "вот денется ?  \n",
            "да , старик .  \n",
            "давай и надо ?  \n",
            "да .  \n",
            "а чей это храм ? какого бога ?  \n",
            "бога единого .  \n",
            "да все .  \n",
            "а чей это ?  \n",
            "надо такая место взял ?  \n",
            "взял .  \n",
            "можешь , как хоть кофе , здравствуйте , быть , чего ?  \n",
            "леся галина его !  \n",
            "у меня .  \n",
            "сколько времени ?  \n",
            "так темно .  \n",
            "врешь .  \n",
            "а когда вы ж фигура ?  \n",
            "не то слово .  \n",
            "я знаю .  \n",
            "а тут спал ?  \n",
            "слышал , сынок .  \n",
            "она что то разбила ?  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С человеком"
      ],
      "metadata": {
        "id": "2bB3JakgGBBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,200):\n",
        "  InputSentence=input()\n",
        "  StartSentnce=ChatAnswer(InputSentence)\n",
        "  StartSentnce=StartSentnce.replace('<end>', '')\n",
        "  #print(\"-\"+InputSentence)\n",
        "  print(StartSentnce)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "Khn7Or5I0gk9",
        "outputId": "14120edf-8e48-4b6d-9ac7-98798a2418ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Зачем?\n",
            "надо .  \n",
            "Что надо?\n",
            "это вам .  \n",
            "Что мне?\n",
            "кажется , пожалуйста .  \n",
            "Спасибо.\n",
            "да ну ? что ? плакат ? плакат ? что же ты хочешь ? плакат ? что же ты хочешь ? плакат \n",
            "Какой плакат?\n",
            "вы ничего .  \n",
            "Почему?\n",
            "тебе не понять , сосунок .  \n",
            "Да ну.\n",
            "едем дальше ?  \n",
            "поехали\n",
            "а где ?  \n",
            "что где\n",
            "а а где ?  \n",
            "а тут\n",
            "вот готов .  \n",
            "ну и хорошо\n",
            "а вот и прелестно .  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-233-02bed796dd1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mInputSentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mStartSentnce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChatAnswer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mStartSentnce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStartSentnce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(\"-\"+InputSentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С вниманием более осмыслено, с человеком лучше чем сам с собой."
      ],
      "metadata": {
        "id": "cRboY38D1cHv"
      }
    }
  ]
}